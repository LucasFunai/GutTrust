GutTrust AI Model overview

We will assume, that the user will feel "normal" most of the days, and has equal chance to feel bad, or feel good.

the rates should look like this -> 1 2 3 4 5 6 7 [1 2 3 5 3 2 1] Total: 17

when normalized, it looks like this -> [0.058 0.117 0.176 0.294 0.176 0.117 0.058] Total:1

When a "risk rate" is 2, it means that the total chance for the score to go to the lower half is doubled.

1 is subtracted from the rate, and the difference is multiplied by 6. The product is then distributed to add to the chance of a bad score.

This is done BEFORE normalizing.

This is the rates with a food with risk-rate at 2.

[2 4 6 5 3 2 1] Total: 23

And normalized: [0.08 0.173 0.260 0.217 0.130 0.08 0.04] Total 1

The sum of the first 3 scores: 0.52
 
The sum of the last 3 scores: 0.26

Since the first 3 scores has twice the weight of the last 3, the chance of a bad score has doubled.

In this case, the AI should be predicting about 40% of bad scores. Anything above 60% would be overfitting.

With 16 foods, and the default risk-rate being 1, the AI has no way to reasonably predict scores with only one food changing the rate.

The default chance of guessing every score correctly with no clues, is 1 / (7 * 7 * 7) = apx. 0.2%

  / 5 /5/  5 /	/ 6  /6/5/  6  /	
[1 2 3 5 3 2 1] [2 4 6 5 3 2 1]

Before, there was Bad, Normal, and Good. Their chances were equal. Now there is twice the weight on Bad. 
Since there were 3 groups, now there is weight for 4, but with 2 of them belonging to the same group, we can say that the guessing is 4/3 times (33%) easier.

Now the chance to predict scores is 0.2% * 133% = 0.266%

This means the AI will NEVER, EVER predict a score correctly. So Categorical accuracy is not a valid metric.

We need to use a metric with a way to know how far off we are.

TIL (9/13/2021): CosineSimilarity, is a mathematical operation to determine how "Similar" two objects are, by creating a graph using multidimensional vectors, drawing a line from
zero point and going both to the prediction, and to the answer. Then it calculates the cosine. (Dunno what exactly it does, just that it makes the function non-linear, and prevents
binary results to reduce dying neurons.)
It is not usable for this AI.

CategoricalHinge seems best for this. So this AI's output should be a 2D vector.

Instead of the AI trying to return the actual score, it should return the possibility for each score.
Since this AI is not trying to predict the EXACT score, probabilities are not only enough, but better for the sake of better metrics.
Afterall it makes no sense to put accuracy as a metric when it does not matter.

We will use a custom accuracy function, which determines if it predicted the score being in the bottom 1/3, the middle 1/3, or the top 1/3.

use the mean wrapper for it.


Since each food has its own risk score, having one neuron for each seems reasonable. Afterall the AI only has to learn the hidden risk rates behind each food.

Also there are not many elements to deduce the score from, so dropout and gaussian noise seems like a risky layer to add. Might increase the bias.

